<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"„ÄÄ/>
<title>Ziyang Ma (È©¨Â≠êÈò≥)</title>
</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ&hl=zh-CN&oi=ao">Scholar</a></div>
<div class="menu-item"><a href="https://github.com/ddlBoJack">GitHub</a></div>
<div class="menu-item"><a href="https://www.linkedin.com/in/ziyang-ma">Linkedin</a></div>
<div class="menu-item"><a href="https://blog.csdn.net/weixin_45019478">Blog</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Ziyang Ma (È©¨Â≠êÈò≥)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="index.html"><img src="pic/avatar.jpeg" alt="alt text" width="120px" height="160px" /></a>&nbsp;</td>
<td align="left"><p>Ph.D. student,<br />
Shanghai Jiao Tong University.<br />
800 Dongchuan RD. Minhang District,<br />
Shanghai, China.<br /> 
E-mail: <a href="mailto:zym.22@sjtu.edu.cn">zym.22@sjtu.edu.cn</a></p>
</td></tr></table>


<h2>Biography</h2>
<p>Hiüëã nice to meet you!</p>
<p>Currently I am a Ph.D. student of Shanghai Jiao Tong University (SJTU) and SJTU Artificial Intelligence Institute, and a member in <a href="https://x-lance.sjtu.edu.cn/">Cross Media (<b>X-</b>) <b>Lan</b>guage Intelligen<b>ce</b> Lab (X-LANCE)</a> of the Department of Computer Science and Engineering, co-supervised by Prof. <a href="https://chenxie95.github.io/">Xie Chen</a>, <a href="https://x-lance.sjtu.edu.cn/en/members/yanmin-qian">Yanmin Qian</a> and working closely with Prof. <a href="https://x-lance.sjtu.edu.cn/members/kai_yu">Kai Yu</a>. As the first Ph.D. supervised by Prof. Chen, I will try my best in the next five exciting years! üí™</p>
<p>I was a research assistant at <a href="https://ilearn.qd.sdu.edu.cn/"><b>I</b>nte<b>L</b>ligent m<b>e</b>di<b>a</b> <b>r</b>esearch ce<b>n</b>ter (iLearn)</a>, working closely with Prof. <a href="https://xuemengsong.github.io/">Xuemeng Song</a> and <a href="https://liqiangnie.github.io/">Liqiang Nie</a> during my undergraduate years.<p>
<p>My research usually follows the <a href="https://wikipedia.org/wiki/KISS_principle">KISS</a> philosophy. My recent work focuses on speech, language, audio and music processing with Self-Supervised Learning (SSL) and Large Language Model (LLM). If you are also interested, please feel free to contact me.</p>

<h3>Education</h3>
<ul>
<li><p>Ph.D., Computer Science and Engineering, Shanghai Jiao Tong University, 2022.09-Now</p>
</li>
<li><p>B.E., Computer Science and Technology, Shandong University, 2018.09-2022.06</p>
</li>
</ul>

<h3>Interests</h3>
<ul>
<li><p>Self-Supervised Learning</p>
</li>
<li><p>Speech Processing</p>
</li>
<li><p>Natural Language Processing</p>
</li>
<li><p>Multimedia and Multimodal</p>
</li>
</ul>

<h3>NEWS</h3>
<!-- <img src="pic/new_gif.gif" alt="alt text"/> -->
<ul>
<li><p>[2023.9] 2 papers were accpeted by IEEE ASRU2023.</p></li>
<li><p>[2023.8] <img src="pic/new_gif.gif" alt="alt text"/> <a href="https://arxiv.org/abs/2211.07321">MT4SSL</a> was nominated in <font color=red>ISCA Interspeech Best Student Paper Shortlist</font>. Congrats!</p></li>
<li><p>[2023.5] 4 papers were accpeted by ISCA INTERSPEECH2023.</p></li>
<li><p>[2023.2] 2 papers were accpeted by IEEE ICASSP2023.</p></li>
<li><p>[2022.11] Check out our <a href="https://github.com/ddlBoJack/MT4SSL">Repo</a> for MT4SSL, a multi-task learning framework for self-supervised learning.</p></li>
<li><p>[2022.10] Check out our <a href="https://github.com/Moon0316/T2A">Repo</a> for Few-Shot Learning for Talking Face System. We train a talking face with only ten randomly picked utterances for about one minute! </p></li>
<li><p>[2022.09] We won 3rd place in <a href="http://www.aiwin.org.cn/competitions/69">Avatar Track of AIWIN</a>, held by WAIC2022.[<a href="https://mp.weixin.qq.com/s/3wuZG4I4PbXW9WmAfuVFwQ">Report</a>][<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]</p></li>
<li><p>[2022.09] I start to pursue my Ph.D. at Shanghai Jiao Tong University.</p></li>
<li><p>[2022.06] I graduated with a Bachelor's Honours Degree! Thanks to everyone who helped me along the way and to myself.</p></li>
<li><p>[2022.05] Check out our <a href="https://github.com/ddlBoJack/Awesome-Speech-Pretraining">Repo</a> for Self-Supervised Learning and Pre-Training on Speech.</p></li>
</ul>


<h2>Research</h2>
<h3>Publications</h3>
<p>Thanks to all the collaborators for their great work!</p>

<p><b>Models and Methods for Speech SSL</b></p>
<ul>
<li><p>
<i>Guanrou Yang, <b>Ziyang Ma</b>, Zhisheng Zheng, Yakun Song, Zhikang Niu, Xie Chen*.</i><br>
<b>Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.</b><br>
in <i>IEEE Automatic Speech Recognition and Understanding Workshop, 2023.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Zhisheng Zheng, Guanrou Yang, Yu Wang, Chao Zhang, Xie Chen*.</i><br>
<b>Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation.</b><br>
in <i>INTERSPEECH, 2023.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Zhisheng Zheng, Changli Tang, Yujin Wang, Xie Chen*.</i><br>
<b>MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets.</b><br>
<font color=red>Oral & Best Student Paper Shortlist</font> in <i>INTERSPEECH, 2023.</i><br>
</p></li>
<li><p>
<i>Zhuoyuan Yao, Shuo Ren, Sanyuan Chen, <b>Ziyang Ma</b>, Pengcheng Guo, Lei Xie*.</i><br>
<b>TESSP: Text-Enhanced Self-Supervised Speech Pre-Training.</b><br>
in <i>arXiv, 2022.</i><br>
</p></li>
</ul>


<p><b>Applications on Speech SSL</b></p>
<ul>
<li><p>
<i><b>Ziyang Ma</b>, Wen Wu, Zhisheng Zheng, Yiwei Guo, Qian Chen, Shiliang Zhang, Xie Chen*.</i><br>
<b>Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition.</b><br>
in <i>arXiv, 2023.</i><br>
</p></li>
<li><p>
<i>Yifan Yang, Feiyu Shen, Chenpeng Du, <b>Ziyang Ma</b>, Kai Yu, Daniel Povey*, Xie Chen*.</i><br>
<b>Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS.</b><br>
in <i>arXiv, 2023.</i><br>
</p></li>
<li><p>
<i>Yujin Wang, Changli Tang, <b>Ziyang Ma</b>, Zhisheng Zheng, Xie Chen, Weiqiang Zhang*.</i><br>
<b>Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition.</b><br>
in <i>IEEE Automatic Speech Recognition and Understanding Workshop, 2023.</i><br>
</p></li>
<li><p>
<i>Zhisheng Zheng, <b>Ziyang Ma</b>, Yu Wang, Xie Chen*.</i><br>
<b>Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition.</b><br>
in <i>INTERSPEECH, 2023.</i><br>
</p></li>
<li><p>
<i>Xie Chen*, <b>Ziyang Ma</b>, Changli Tang, Yujin Wang, Zhisheng Zheng.</i><br>
<b>Front-End Adapter: Adapting Front-End Input of Speech based Self-Supervised Learning for Speech Recognition.</b><br>
in <i>IEEE International Conference on Acoustics, Speech and Signal Processing, 2023.</i><br>
</p></li>
</ul>

<p><b>Speech Synthesis and Generation</b></p>
<ul>
<li><p>
<i>Yiwei Guo, Chenpeng Du, <b>Ziyang Ma</b>, Xie Chen, Kai Yu*.</i><br>
<b>VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching.</b><br>
in <i>arXiv, 2023.</i><br>
</p></li>
<li><p>
<i>Zheng Liang, Zheshu Song, <b>Ziyang Ma</b>, Chenpeng Du, Kai Yu, Xie Chen*.</i><br>
<b>Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation.</b><br>
in <i>INTERSPEECH, 2023.</i><br>
</p></li>
</ul>

<p><b>Multimedia and Multimodal</b></p>
<ul>
<li><p>
<i>Qi Chen, <b>Ziyang Ma</b>, Tao Liu, Xu Tan, Qu Lu, Kai Yu, Xie Chen*.</i><br>
<b>Improving Few-Shot Learning for Talking Face System with TTS Data Augmentation.</b><br>
in <i>IEEE International Conference on Acoustics, Speech and Signal Processing, 2023.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Xianjing Han, Xuemeng Song*, Yiran Cui, Liqiang Nie.</i><br>
<b>Hierarchical deep residual reasoning for temporal moment localization.</b><br>
<font color=red>Oral</font> in <i>ACM Multimedia Asia, 2021.</i><br>
</p></li>
</ul>

<p>Note: * indicates the corresponding author.</p>
<!-- <p><a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ&hl=zh-CN&oi=ao">Full list of publications in Google Scholar</a>.</p> -->

<h3>Experiences</h3>
<p>Research Intern, NLC Group, <a href="https://www.msra.cn/">Microsoft Research Asia(MSRA)</a>, 2022.02-2022.08</p>
<ul>
<li><p>Investigate joint pre-training of speech and text to help improve the accuracy of ASR and other downstream tasks.</p>
</li>
<li><p>Led by <a href="http://gitnlp.org/">Furu Wei</a>, supervised by <a href="https://rshuo.github.io/">Shuo Ren</a> and <a href="https://scholar.google.com/citations?user=6mNya-wAAAAJ&hl=en">Shujie Liu</a>, and working closely with <a href="https://sites.google.com/view/wuyu/home">Yu Wu</a> and <a href="https://long-zhou.github.io/">Long Zhou</a>.</p>
</li>
</ul> 
<p>Research Intern, Video Group, <a href="https://research.megvii.com/">MEGVII Research</a>, 2021.04-2021.06</p>
<ul>
<li><p>Investigate re-identification of vehicle with Transformer architecture.</p>
</li>
<li><p>Supervised by <a href="https://scholar.google.com/citations?user=bvPDWO4AAAAJ&hl=zh-CN&oi=sra">Chi Zhang</a>.</p>
</li>
</ul>
<p>Research Assistant, <a href="https://ilearn.qd.sdu.edu.cn/">InteLligent media research center (iLearn)</a>, Shandong University, 2020.09-2021.09</p>
<ul>
<li><p>My work focused on temporal moment localization in untrimmed videos.</p>
</li>
<li><p>Supervised by <a href="https://xuemengsong.github.io/">Xuemeng Song</a> and <a href="https://liqiangnie.github.io/">Liqiang Nie</a>.</p>
</li>
</ul>

<h3>Academic Service</h3>
<p><b>Reviewer</b></p>
<ul>
<li><p>IEEE International Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP)</p>
</li>
<li><p>AAAI Conference on Artificial Intelligence</p>
</li>
<li><p>ACM International Conference on Multimedia (ACM MM)</p>
</li>
</ul>
<!-- <p><a href="https://publons.com/researcher/3034188/xiuze-zhou/">More details in Publons</a></p> -->


<h2>Accomplishments</h2>
<h3>Awards</h3>
<ul>
<li><p>Excellent Graduate, Department of Education, Shandong Province, China, 2022.06</p>
</li>
<li><p>"Intelligent Pedestal" Scholarship, Huawei, 2021.12</p>
</li>
<li><p>SIGMM Student Travel Grant, ACM, 2021.11</p>
</li>
<li><p>National Scholarship, Ministry of Education, China, 2021.10</p>
</li>
</ul>

<h3>Competitions</h3>
<ul>
<li><p>3rd in <a href="https://dcase.community/challenge2023/task-sound-event-detection-with-soft-labels">DCASE2023 Challenge Task4b: Sound Event Detection with Soft Labels</a>, IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events, 2023.06.</p>
</li>
<li><p>3rd in <a href="http://www.aiwin.org.cn/competitions/69">Avatar Track of AIWIN</a>, the 5th World Artificial Intelligence Conference(WAIC2022), Shanghai, China, 2022.09.[<a href="https://mp.weixin.qq.com/s/3wuZG4I4PbXW9WmAfuVFwQ">Report</a>][<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]</p>
</li>
<li><p>Finalist(Top 284 in 26112 teams) in <a href="https://www.comap.com/undergraduate/contests/mcm/contests/2021/results/">Contest in Modeling (MCM)</a>, Consortium for Mathematics and Its Application, America, 2021.02</p>
</li>
<li><p>First Prize(Top 293 in 45689 teams) in <a href="http://www.mcm.edu.cn/">Contemporary Undergraduate Mathematical Contest in Modeling (CUMCM)</a>, China Society for Industrial and Applied Mathematics, China, 2020.09</p>
</li>
</ul>

<h3>Activities</h3>
<ul>
<li><p>[<a href="https://mp.weixin.qq.com/s/ckk2GD3aqJIVoN5ndLIOfw">SpeechHome Invited Talk</a>]: INTERSPEECH 2023 Pre-presentation </a>, 2023.07</p>
</li>
<li><p>[<a href="https://www.bilibili.com/video/BV1Ld4y177M9">AI TIME PhD Debate</a>]: PhD Debate on digital human and metaverse</a>, 2023.01</p>
</li>
<li><p>[<a href="https://www.bilibili.com/video/BV1iV4y137RC">Datawhale Invited Talk</a>]: How to conduct audio-driven talking head? An introduction and solution sharing</a>, 2022.11</p>
</li>
<li><p>Member of <a href="https://datawhale.club/">Datawhale</a>, 2022.09-Now</p>
</li>
<li><p>Teaching Assistant, Computer Science and Technology, Shandong University, 2021.03-2021.06</p>
</li>
<li><p>Member of Elite Class, Computer Science and Technology, Shandong University, 2020.09-2022.06</p>
</li>
</ul>

<!-- <p><br><a href="cv/cv.pdf">A brief cv</a>.</p> -->
<center>
<a href="https://clustrmaps.com/site/1bonu" title="Visit tracker"><img src="https://clustrmaps.com/map_v2.png?cl=ffffff&w=300&t=tt&d=6yrMMIqIUJ1GT9nydxBO8cdtx5nw8iVTjA_d5hAcBcY&co=2d78ad&ct=ffffff" /></a>
</center>
</td>
</tr>
</table>
</body>
</html>